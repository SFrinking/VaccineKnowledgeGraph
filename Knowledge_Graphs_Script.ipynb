{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import unidecode\n",
    "from collections import Counter\n",
    "\n",
    "PATH = 'C:\\\\Users\\\\sanne\\\\Documents\\\\Master\\\\Applied Text Mining\\\\System\\\\'\n",
    "VADER = 'Sent_polarity3\\\\'\n",
    "ALLEN = 'conll-allen\\\\conll-allen-nlp\\\\'\n",
    "OPINION = 'Opinions\\\\output\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_sent_ids():\n",
    "    '''Loading output of script saved as a picklefile. This script extracts relevant frames using FrameNet\n",
    "    from the sentences provided in the NAF files'''   \n",
    "    all_relevant_sent_ids = pickle.load(open(PATH + \"all_relevant_sent_ids.pickle\", \"rb\"))\n",
    "    for key in all_relevant_sent_ids:\n",
    "        all_relevant_sent_ids[key.replace('&', '_')] = all_relevant_sent_ids.pop(key) #necessary for grouping function\n",
    "    for key in all_relevant_sent_ids:\n",
    "        all_relevant_sent_ids[key.replace('\\'', '_')] = all_relevant_sent_ids.pop(key) #necessary for grouping function\n",
    "    return all_relevant_sent_ids\n",
    "\n",
    "def change_format_for_grouping(data):\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.reset_index(inplace=True)\n",
    "    data.index = data.index.set_names(['ind'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(foldername, delimiter= ',', make_dict = 'no'): \n",
    "    '''loads CSV files of given folder and returns it as a dataframe\n",
    "    \n",
    "    Keyword argument = make_dict, default = no. If yes then a dictionary is made with the identifiers as keys\n",
    "    and the words as values. This dictionary is ascribed as value to the filename as key in a all_identifiers_dict\n",
    "    \n",
    "    Returns the dataframe of all files and dictionary of all files'''\n",
    "    total_data = pd.DataFrame()\n",
    "    basepath = Path(PATH + foldername)\n",
    "    opinionfiles_in_basepath = basepath.iterdir()\n",
    "    all_identifiers_dict = dict()\n",
    "    for path_to_file in opinionfiles_in_basepath:\n",
    "        if path_to_file.is_file():  # check if the item is not a subdirectory\n",
    "            data = pd.read_csv(path_to_file, encoding = 'ANSI', error_bad_lines=False, sep=delimiter)\n",
    "            filename = os.path.split(path_to_file)[-1]    \n",
    "            filename = filename.split('.')[0]\n",
    "            if filename.endswith('-opinions'):\n",
    "                filename = filename[:-9]\n",
    "            data['file'] = filename\n",
    "            if make_dict == 'yes':\n",
    "                file_dict = make_identifiers_dict_per_file(data)\n",
    "                all_identifiers_dict[filename] = file_dict\n",
    "                #all_identifiers_dict = pickle.load(open(PATH + \"conll_allen_dict.pickle\", \"rb\"))\n",
    "            total_data = pd.concat([total_data, data])\n",
    "    return total_data, all_identifiers_dict\n",
    "\n",
    "def find_most_frequent(polarity_list_sent):\n",
    "    '''returns the most frequent polarity of a list of polarities from one sentence, if there are positive and negative \n",
    "    occur as often as each other, neutral is returned'''\n",
    "    polarity_counter = Counter(polarity_list_sent)\n",
    "    highest = polarity_counter.most_common(2)\n",
    "    try:\n",
    "        if highest[0][1] == highest [1][1]: #positive and negative occur equal\n",
    "            polarity = 'neutral'\n",
    "        else:\n",
    "            polarity = highest[0][0]\n",
    "    except:\n",
    "        polarity = highest[0][0]\n",
    "    return polarity\n",
    "\n",
    "def build_perspective_graph(input, conll_allen, conll_allen_dict, all_relevant_sent_ids, counter = 'no'):\n",
    "    '''loops through concated inputfile, takes filename, sentence id, calculates polarity, sentence text, finds source\n",
    "    cue content and builds this into a knowledge graph dictionary'''\n",
    "    knowledge_graph = dict()\n",
    "    file_sent_id_list = []\n",
    "    file_list = []\n",
    "    \n",
    "    for index, row in input.iterrows():  #must loop through stan's file\n",
    "        file = row['file']\n",
    "        sent_id = row['sent_id']\n",
    "        file_sent_id = file + str(sent_id)\n",
    "        if counter == 'no':\n",
    "            sent_text = row['sent_text']\n",
    "        else:  #if input is the opinion data should be row['text']\n",
    "            sent_text = row['text']\n",
    "        polarity = row['polarity']\n",
    "        \n",
    "        if file not in file_list:   #if new file, make a new file dictionary, has sentence ids as keys\n",
    "            file_dict = dict() \n",
    "            file_list.append(file)  # keeps track which files are done\n",
    "        if str(sent_id) not in all_relevant_sent_ids[file]:   #checking if new sent_id is relevant if not add previous one to knowledge graph\n",
    "            add_to_knowledge_graph_if_last(knowledge_graph, file_dict, input, index, file) #adds the last because of conditions in function\n",
    "            continue\n",
    "        if file_sent_id in file_sent_id_list:   #if same sentid, add the previous one to the knowledge graph if last\n",
    "            if counter == 'yes':   #in naf opinion files, there are multiple polarities for one sentence, in vader not\n",
    "                try:\n",
    "                    polarity_list_sent.append(polarity)\n",
    "                except:\n",
    "                    polarity_list_sent = []\n",
    "                    polarity_list_sent.append(polarity)\n",
    "            polarity = find_most_frequent(polarity_list_sent)\n",
    "            add_to_knowledge_graph_if_last(knowledge_graph, file_dict, input, index, file) #adds if the last sentence because of conditions in function\n",
    "        else:   #sent id is new\n",
    "            polarity_list_sent = []\n",
    "            file_sent_id_list.append(file_sent_id)  #keeps track which files+sentences are done\n",
    "        \n",
    "        sent_dict = get_source_cue_content(file, sent_id, conll_allen, conll_allen_dict)\n",
    "        if not sent_dict:\n",
    "            add_to_knowledge_graph_if_last(knowledge_graph, file_dict, input, index, file)\n",
    "            continue\n",
    "        else:    \n",
    "            sent_dict['polarity'] = polarity.strip()\n",
    "            sent_dict['sent_text'] = sent_text.strip()\n",
    "            file_dict[sent_id] = sent_dict\n",
    "        \n",
    "            polarity_list_sent = add_to_knowledge_graph_if_last(knowledge_graph, file_dict, input, index, file)\n",
    "            print(knowledge_graph)\n",
    "    \n",
    "    #with open('knowledge_graph_1.pickle', 'wb') as kg:\n",
    "        #pickle.dump(knowledge_graph, kg, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return knowledge_graph\n",
    "\n",
    "\n",
    "def add_word_to_id_in_dict(id, row, identifiers_dict):\n",
    "    '''finds identifier, splits it and adds all words with that idnumber to a dictionary'''\n",
    "    if id is not '_' and isinstance(id, str):    #there is no # in source and content, not necassary to split\n",
    "        number = id.split('-')[-1]         \n",
    "        token_word = row['word'] + ' '\n",
    "        token_word = token_word.encode('latin1',errors='ignore').decode('cp1252',errors='ignore')\n",
    "        token_word = re.sub('[^a-zA-Z0-9 \\n\\.]', '', token_word)\n",
    "        try:\n",
    "            token_word = token_word.encode('latin1').decode('Windows-1252')\n",
    "        except:\n",
    "            try:\n",
    "                token_word = token_word.encode('cp1252').decode()\n",
    "            except:\n",
    "                print(token_word)\n",
    "        if identifiers_dict.get(number):\n",
    "            already_added_word = identifiers_dict.get(number)\n",
    "        else:\n",
    "            already_added_word = ''\n",
    "        already_added_word += token_word\n",
    "        identifiers_dict[number] = already_added_word\n",
    "    return identifiers_dict\n",
    "\n",
    "def add_content_to_ids_in_dict(content_id, row, identifiers_dict):\n",
    "    '''finds identifier, splits it and adds all words with that idnumber to a dictionary'''\n",
    "    if content_id is not '_' and isinstance(content_id, str):\n",
    "        if '#' in content_id:    #I-content-21648 #B-content-21676:21674-Cue_21675-Source\n",
    "            content_ids = re.split('#|:', content_id)\n",
    "            for part in content_ids:\n",
    "                if part.startswith('B') or part.startswith('I'):\n",
    "                    content_number = part.split('-')[-1] \n",
    "        else:  #B-content-21485:21483-Source_21484-Cue_21486-Source                \n",
    "            content_number = content_id.split(':')[0].split('-')[-1]\n",
    "        try:\n",
    "            content_word = row['word'] + ' '\n",
    "            content_word = content_word.encode('latin1',errors='ignore').decode('cp1252',errors='ignore')\n",
    "            content_word = re.sub('[^a-zA-Z0-9 \\n\\.]', '', content_word)\n",
    "        except:\n",
    "            content_word = 'NaN'  #mistake in NAF files where word is type NaN\n",
    "        \n",
    "        if identifiers_dict.get(content_number):                \n",
    "            already_added_content = identifiers_dict.get(content_number)\n",
    "        else:\n",
    "            already_added_content = ''\n",
    "        already_added_content += content_word                \n",
    "        identifiers_dict[content_number] = already_added_content\n",
    "    return identifiers_dict\n",
    "\n",
    "\n",
    "def make_identifiers_dict_per_file(data):\n",
    "    '''loop through file and save all words to source, cue, content as words\n",
    "    Returns the dictionary of that file and adds it with filename as key to total dictionary'''\n",
    "    identifiers_dict = dict()\n",
    "    for i, r in data.iterrows():\n",
    "        source = r['attr_source']\n",
    "        cue = r['attr_cue']\n",
    "        content = r['attr_content']\n",
    "        \n",
    "        identifiers_dict = add_word_to_id_in_dict(source, r, identifiers_dict)\n",
    "        identifiers_dict = add_word_to_id_in_dict(cue, r, identifiers_dict)\n",
    "        identifiers_dict = add_content_to_ids_in_dict(content, r, identifiers_dict) \n",
    "    return identifiers_dict\n",
    "\n",
    "def get_source_cue_content(file, sent_id, conll_allen, conll_allen_dict):\n",
    "    '''Loops through the conll_allen rows of the file and sentence id of which the source cue content is wanted, looks for the \n",
    "    identifier that specifies the source cue content triple, takes their ids and looks them up in the id-dictionary of that file\n",
    "    Output = a dictionary of the sentence with source, cue and value as keys \n",
    "    that gets assigned to sentence id outside the function, if id does not excist it goes to next iteration'''\n",
    "    sentence_dict = dict()\n",
    "    selection = conll_allen.loc[(conll_allen['file'] == file)&(conll_allen['sent_id'] == sent_id)]\n",
    "    for i, r in selection.iterrows():\n",
    "        identifier = r['attr_content']\n",
    "        if '#' in identifier:                     #I-content-21648#B-content-21676:21674-Cue_21675-Source\n",
    "            list_ids = identifier.split('#')\n",
    "            for id in list_ids:\n",
    "                if id.startswith('B') and 'Cue' in id:  #only take triples where there is a cue\n",
    "                    wanted_id = id                #B-content-21676:21674-Cue_21675-Source\n",
    "                else:\n",
    "                    continue\n",
    "            continue\n",
    "        elif ':' in identifier and 'Cue' in identifier: #only take triples where there is a cue\n",
    "            wanted_id = identifier    #B-content-21485:21483-Source_21484-Cue_21486-Source\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        id_splitted = re.split(':|_', wanted_id)\n",
    "        for id in id_splitted:\n",
    "            if 'content' in id:\n",
    "                content_wanted = id.split('-')[-1]    #B-content-21485\n",
    "            elif 'Source' in id:\n",
    "                source_wanted = id.split('-')[0]      #21483-Source\n",
    "                continue                              #only takes the first source\n",
    "            elif 'Cue' in id:\n",
    "                cue_wanted = id.split('-')[0]         #21484-Cue\n",
    "                \n",
    "        try:\n",
    "            source = conll_allen_dict[file].get(source_wanted)  #lookup id in id dictionary of this file\n",
    "            source_wanted = ''\n",
    "            if source is None:\n",
    "                source = 'Unknown'\n",
    "        except:\n",
    "            source= 'Unknown'\n",
    "        cue = conll_allen_dict[file].get(cue_wanted) #lookup id in dictionary of this file\n",
    "        cue_wanted = ''\n",
    "            \n",
    "        try:\n",
    "            content = conll_allen_dict[file].get(content_wanted) #lookup id in dictionary of this file\n",
    "            if content is None:\n",
    "                content = 'Unknown'\n",
    "            content_wanted = ''\n",
    "        except:\n",
    "            content = 'Unknown'\n",
    "            \n",
    "        sentence_dict['source'] = source.strip() #needs.strip() but gives error if None. 5605 in ABC not in id dict\n",
    "        sentence_dict['cue'] = cue.strip() #needs .strip() but gives error if None. 5605 in ABC not in id dict\n",
    "        sentence_dict['content'] = content.strip() #needs .strip() but gives error if None. 5605 in ABC not in id dict\n",
    "    return sentence_dict\n",
    "\n",
    "def make_output_csv(knowledge_graph, name):\n",
    "    data = pd.DataFrame.from_dict({(i,j): knowledge_graph[i][j] \n",
    "                           for i in knowledge_graph.keys() \n",
    "                           for j in knowledge_graph[i].keys()},\n",
    "                       orient='index')\n",
    "    data.to_csv(name, index=True)\n",
    "\n",
    "def add_to_knowledge_graph_if_last(knowledge_graph, file_dict, opinion_data, index, file): \n",
    "    '''Checks whether it's the last sentence of the document or of a file if true, adds file_dict to knowledge graph'''\n",
    "    if index == opinion_data.iloc[-1, 0]:\n",
    "        knowledge_graph[file] = file_dict\n",
    "    elif file != opinion_data.loc[index+1, 'file']:\n",
    "        knowledge_graph[file] = file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    all_relevant_sent_ids = get_relevant_sent_ids()\n",
    "    opinion_data, opinion_dict = load_files(OPINION)\n",
    "    vader_data, vader_dict = load_files(VADER)\n",
    "    change_format_for_grouping(opinion_data)\n",
    "    change_format_for_grouping(vader_data)\n",
    "    conll_allen, conll_allen_dict = load_files(ALLEN, delimiter = '\\t', make_dict = 'yes')\n",
    "    #with open('conll_allen_dict_fullcontent.pickle', 'wb') as dct:\n",
    "    #    pickle.dump(conll_allen_dict, dct, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #with open('conll_allen_fullcontent.pickle', 'wb') as cnl:\n",
    "    #    pickle.dump(conll_allen, cnl, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #conll_allen = pickle.load(open(PATH + \"conll_allen.pickle\", \"rb\"))\n",
    "    #conll_allen_dict = pickle.load(open(PATH + \"conll_allen_dict.pickle\", \"rb\"))\n",
    "    first_knowledge_graph = build_perspective_graph(opinion_data, conll_allen, conll_allen_dict, \n",
    "                                                    all_relevant_sent_ids, counter = 'yes')\n",
    "    second_knowledge_graph = build_perspective_graph(vader_data, conll_allen, conll_allen_dict, all_relevant_sent_ids)\n",
    "    make_output_csv(first_knowledge_graph, 'First_Knowledge_Graph.csv')\n",
    "    make_output_csv(second_knowledge_graph, 'Second_Knowledge_Graph.csv')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
